# Procedural Terrain Generator — Implementation Flow

## Project Overview

A React app that uses Three.js to procedurally generate and render a 3D terrain mesh driven by Perlin noise. The terrain is lit by a custom GLSL shader, colored by biome zones based on elevation, and controlled through a live parameter panel. Every step is designed to teach a specific part of the geometry → shader → render pipeline, building toward a scene where you own every part of the rendering stack.

---

## Tech Stack

- **React 18+ with Vite** — application shell and hot reloading
- **Three.js (raw)** — no React Three Fiber, so you manage the Three.js lifecycle manually inside `useEffect`. This is intentional. R3F abstracts the renderer, scene, and loop away from you. You want to see those pieces and wire them yourself.
- **GLSL (inline shader strings)** — vertex and fragment shaders written by hand, not generated by Three.js material classes
- **RawShaderMaterial** — not `ShaderMaterial`. The Raw variant skips Three.js's automatic uniform and attribute injection, forcing you to declare everything yourself. This is as close to writing raw WebGL shaders as you can get while still using Three.js for scene management.
- **simplex-noise (npm)** — CPU-side noise library for the initial displacement step

Why not `MeshStandardMaterial` with a displace map? Because that hides the shader pipeline completely. The goal is to own the vertex and fragment stages — you need `RawShaderMaterial` for that.

---

## File Structure

```
src/
├── App.jsx                          # Layout: viewport left, control panel right
├── main.jsx                         # Entry point
│
├── scene/
│   ├── SceneManager.js              # Core: renderer, scene, camera, animation loop
│   ├── TerrainMesh.js               # Builds PlaneGeometry + displaces vertices + applies material
│   └── TerrainInspector.js          # Reads live WebGL state for the debug panel
│
├── noise/
│   ├── PerlinNoise.js               # Custom Perlin implementation with seeded permutation table
│   └── fbm.js                       # Fractal Brownian Motion — stacks octaves of noise
│
├── shaders/
│   ├── terrain.vert.js              # Vertex shader: passes vHeight + vNormal as varyings
│   └── terrain.frag.js              # Fragment shader: biome color mapping + Lambertian lighting
│
├── components/
│   ├── Viewport.jsx                 # <canvas> element + mounts SceneManager
│   ├── ControlPanel.jsx             # Sliders for noise params, toggles for render modes
│   └── DebugPanel.jsx               # Live display: FPS, vertex count, min/max height, shader uniforms
│
└── hooks/
    ├── useAnimationLoop.js          # requestAnimationFrame hook
    └── useOrbitControls.js          # Minimal mouse orbit: theta, phi, radius → camera.position
```

---

## Build Flow — Step by Step

Each step produces a visible result and teaches a specific WebGL concept. Don't skip ahead — the understanding compounds with each layer.

---

### Step 1 — Canvas Setup and Scene Initialization

**What you're building**
Mount a Three.js renderer onto a `<canvas>` element from inside a React component. Get a solid-color background and a perspective camera rendering nothing.

**What this teaches**
The renderer is your interface to the WebGL context. When you call `new THREE.WebGLRenderer()`, Three.js runs `canvas.getContext('webgl2')` under the hood and stores the raw `gl` object internally. Every Three.js call — creating a mesh, rendering a frame — eventually becomes a sequence of `gl.*` calls to that context.

At this step you also establish the animation loop. The loop is just `requestAnimationFrame` recursing on itself, calling `renderer.render(scene, camera)` every frame. Three.js translates that into:
- setting the viewport
- clearing the color and depth buffers
- issuing draw calls for each mesh in the scene

```js
// SceneManager.js
const renderer = new THREE.WebGLRenderer({ canvas, antialias: true });
renderer.setSize(width, height);
renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));

const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(55, width / height, 0.1, 500);
camera.position.set(0, 60, 80);
camera.lookAt(0, 0, 0);
```

**What connects to the next step**
You have a renderer and an empty scene. The next step puts geometry into that scene — a flat grid of vertices that will become the terrain surface.

---

### Step 2 — PlaneGeometry and the Vertex Buffer

**What you're building**
Create a `PlaneGeometry`, rotate it flat, add it to the scene with a basic `MeshLambertMaterial`. Verify it renders as a flat gray rectangle.

**What this teaches**
`PlaneGeometry(width, height, segmentsX, segmentsY)` creates a rectangular grid of vertices. The segment count is the resolution — `128, 128` produces `(128+1) × (128+1) = 16,641` vertices.

Internally Three.js builds two buffers from this:

- **VBO (Vertex Buffer Object)** — a `Float32Array` of all vertex positions: `[x₀,y₀,z₀, x₁,y₁,z₁, ...]`. This is what gets uploaded to GPU memory.
- **EBO (Element Buffer Object)** — an index array that tells the GPU which three vertices form each triangle. The GPU never stores duplicate vertices; it just looks them up by index.

Both buffers live in GPU VRAM after the first render call. You're not drawing from JavaScript arrays — you're drawing from memory the GPU can access directly.

The `geometry.attributes.position` accessor in Three.js is how you read and write that VBO from JavaScript before it goes to the GPU.

```js
// TerrainMesh.js
const geo = new THREE.PlaneGeometry(60, 60, 128, 128);
geo.rotateX(-Math.PI / 2); // default plane is vertical — rotate to lie flat

const mat = new THREE.MeshLambertMaterial({ color: 0x4a7c59 });
const mesh = new THREE.Mesh(geo, mat);
scene.add(mesh);
```

**What connects to the next step**
The flat grid is in the scene. Now you need to push each vertex's Y value up or down based on a noise function. That means looping the VBO, sampling noise, and writing heights back into it.

---

### Step 3 — Perlin Noise and CPU-Side Vertex Displacement

**What you're building**
Implement the Perlin noise + fBm stack in `PerlinNoise.js` and `fbm.js`. Loop through every vertex in the geometry, sample noise using its X/Z position, and write the result back as Y. Call `computeVertexNormals()` afterward.

**What this teaches**

**Perlin noise** produces a smooth continuous field of values where neighboring samples are correlated. The key internals:
1. Space is divided into an invisible integer grid
2. Each grid corner gets a random gradient vector
3. For any input point, the algorithm dot-products those gradients and blends the results with a smooth easing curve (`6t⁵ - 15t⁴ + 10t³`)

The result is a value between `-1` and `1` that changes smoothly across 2D space.

**fBm (Fractal Brownian Motion)** layers multiple octaves of noise at increasing frequencies and decreasing amplitudes:

```js
// fbm.js
export function fbm(x, y, { octaves, persistence, lacunarity, scale }) {
  let value = 0, amplitude = 1, frequency = scale, maxAmp = 0;
  for (let i = 0; i < octaves; i++) {
    value   += noise(x * frequency, y * frequency) * amplitude;
    maxAmp  += amplitude;
    amplitude  *= persistence;  // each octave contributes less
    frequency  *= lacunarity;   // each octave is higher frequency
  }
  return value / maxAmp; // normalize to [-1, 1]
}
```

**Direct VBO mutation** is the CPU-side displacement step:

```js
// TerrainMesh.js
const positions = geo.attributes.position;

for (let i = 0; i < positions.count; i++) {
  const x = positions.getX(i);
  const z = positions.getZ(i);
  const h = fbm(x, z, noiseParams) * amplitude;
  positions.setY(i, h);
}

positions.needsUpdate = true; // flags the buffer for re-upload to GPU VRAM
geo.computeVertexNormals();   // recalculates normals based on new slopes
```

`positions.needsUpdate = true` is the moment data physically moves from CPU RAM to GPU VRAM. Without it the GPU renders the old flat plane.

`computeVertexNormals()` recalculates each vertex's normal vector by averaging the normals of surrounding triangles. Without this, lighting will be wrong because the GPU still thinks the surface is flat.

**What connects to the next step**
The terrain shape exists. `MeshLambertMaterial` handles lighting automatically, which hides how it actually works. The next step replaces it with a `RawShaderMaterial` so you control the vertex and fragment stages directly.

---

### Step 4 — RawShaderMaterial and the Shader Pipeline

**What you're building**
Replace `MeshLambertMaterial` with `RawShaderMaterial`. Write a vertex shader that passes `vHeight` and `vNormal` as varyings. Write a fragment shader that renders a simple grayscale gradient based on height. Confirm the terrain still renders correctly.

**What this teaches**
When you switch to `RawShaderMaterial`, you take ownership of two programs that run on the GPU:

**The Vertex Shader** runs once per vertex. It receives data from the VBO (position, normal) via `attribute` declarations, and its job is to output `gl_Position` — the final clip-space coordinate of that vertex. It can also pass data downstream via `varying` variables.

**The Fragment Shader** runs once per pixel. It receives the interpolated `varying` values from the vertex shader and outputs `gl_FragColor` — the final RGBA color of that pixel.

**Varyings are interpolated automatically.** If vertex A has `vHeight = 10.0` and vertex B has `vHeight = 2.0`, a pixel exactly halfway between them will receive `vHeight = 6.0`. The GPU does this interpolation for every varying, for every pixel, in parallel. You never do it manually.

```glsl
// terrain.vert.js
precision highp float;

attribute vec3 position;
attribute vec3 normal;

uniform mat4 modelViewMatrix;
uniform mat4 projectionMatrix;
uniform mat3 normalMatrix;

varying float vHeight;
varying vec3  vNormal;
varying vec3  vPosition;

void main() {
  vHeight   = position.y;
  vNormal   = normalize(normalMatrix * normal);
  vPosition = (modelViewMatrix * vec4(position, 1.0)).xyz;

  gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
}
```

```glsl
// terrain.frag.js  (Step 4 — grayscale height visualization)
precision highp float;

uniform float uMinHeight;
uniform float uMaxHeight;

varying float vHeight;

void main() {
  float t = clamp((vHeight - uMinHeight) / (uMaxHeight - uMinHeight), 0.0, 1.0);
  gl_FragColor = vec4(vec3(t), 1.0);
}
```

At this stage the terrain renders as a grayscale gradient — dark at valleys, white at peaks. This confirms the shader pipeline is working before you add any color logic.

**What connects to the next step**
You can see height as brightness. The next step replaces the grayscale output with biome color zones mapped to the same normalized height value.

---

### Step 5 — Biome Color Mapping in the Fragment Shader

**What you're building**
Extend the fragment shader with a `biomeColor(t)` function that maps height to terrain zones: deep water → water → sand → grass → forest → rock → snow. Pass `uMinHeight` and `uMaxHeight` as uniforms from JavaScript.

**What this teaches**
The biome map is a pure fragment shader computation. No CPU logic, no texture lookup — just `if/else` blends running in parallel on the GPU for every pixel simultaneously:

```glsl
// terrain.frag.js
vec3 biomeColor(float t) {
  vec3 deepWater = vec3(0.04, 0.12, 0.28);
  vec3 water     = vec3(0.10, 0.24, 0.50);
  vec3 sand      = vec3(0.76, 0.70, 0.50);
  vec3 grass     = vec3(0.22, 0.48, 0.18);
  vec3 forest    = vec3(0.10, 0.30, 0.10);
  vec3 rock      = vec3(0.45, 0.42, 0.38);
  vec3 snow      = vec3(0.92, 0.95, 1.00);

  if      (t < 0.18) return mix(deepWater, water,  t / 0.18);
  else if (t < 0.25) return mix(water,     sand,   (t - 0.18) / 0.07);
  else if (t < 0.38) return mix(sand,      grass,  (t - 0.25) / 0.13);
  else if (t < 0.58) return mix(grass,     forest, (t - 0.38) / 0.20);
  else if (t < 0.72) return mix(forest,    rock,   (t - 0.58) / 0.14);
  else if (t < 0.88) return mix(rock,      snow,   (t - 0.72) / 0.16);
  return snow;
}
```

`mix(a, b, t)` is GLSL's linear interpolation — same as `lerp`. The transitions between biomes are smooth because each zone uses `mix` rather than a hard cut.

**Uniforms** are how JavaScript sends values into the shader. They're set once per draw call (not per vertex or per pixel) and stay constant for the entire frame:

```js
// TerrainMesh.js
const material = new THREE.RawShaderMaterial({
  vertexShader,
  fragmentShader,
  uniforms: {
    uMinHeight: { value: minH },
    uMaxHeight: { value: maxH },
    uSunDir:    { value: new THREE.Vector3(1.2, 2.0, 0.8).normalize() },
  }
});
```

To update a uniform from JavaScript after construction: `material.uniforms.uMinHeight.value = newMin`. Three.js uploads it to the GPU before the next draw call.

**What connects to the next step**
The terrain has color. But color without lighting is flat and unconvincing — every slope looks the same brightness regardless of angle. The next step adds Lambertian diffuse lighting using the surface normals you already computed.

---

### Step 6 — Lambertian Lighting in the Fragment Shader

**What you're building**
Implement diffuse lighting in the fragment shader using the `vNormal` varying and a `uSunDir` uniform. Add an ambient term so shadowed areas aren't pure black. Optionally add a rim term for silhouette definition.

**What this teaches**
Lambertian diffuse lighting is one dot product:

```glsl
float diffuse = max(dot(vNormal, uSunDir), 0.0);
```

`dot(a, b)` returns the cosine of the angle between two normalized vectors. This means:
- Normal pointing directly at the sun → `cos(0°) = 1.0` → fully lit
- Normal perpendicular to the sun → `cos(90°) = 0.0` → no contribution
- `max(..., 0.0)` clamps negative values — backfaces don't emit negative light

```glsl
// terrain.frag.js
void main() {
  float t     = clamp((vHeight - uMinHeight) / (uMaxHeight - uMinHeight), 0.0, 1.0);
  vec3 color  = biomeColor(t);

  vec3 lightDir = normalize(uSunDir);
  float diff    = max(dot(vNormal, lightDir), 0.0);

  vec3 ambient  = color * 0.35;           // base brightness — no pure black shadows
  vec3 diffuse  = color * diff * 0.85;    // sun contribution

  // Optional: rim light for silhouette pop
  vec3 viewDir  = normalize(-vPosition);
  float rim     = pow(1.0 - max(dot(vNormal, viewDir), 0.0), 3.0) * 0.12;

  gl_FragColor  = vec4(ambient + diffuse + rim, 1.0);
}
```

This is the same core formula that physically-based renderers extend. PBR is Lambertian + specular + occlusion + emissive — but the dot product at the center is identical.

**What connects to the next step**
Lighting is live. The next step adds the control panel — React state wired to uniform values and geometry rebuilds — so you can tweak every noise parameter interactively and see results in real time.

---

### Step 7 — Control Panel and Live Parameter Wiring

**What you're building**
Build the `ControlPanel.jsx` component with sliders for `scale`, `octaves`, `persistence`, `lacunarity`, `amplitude`, `resolution`, and `seed`. Wire each slider to React state. Rebuild the terrain geometry on noise parameter changes. Update shader uniforms on render mode toggles.

**What this teaches**
This step makes the pipeline observable. When you drag the `octaves` slider from 3 to 6 and watch the terrain gain detail, you're seeing exactly what that fBm parameter does to the vertex data.

There are two categories of change here:

**Geometry rebuilds** — changing `scale`, `octaves`, `persistence`, `lacunarity`, `amplitude`, `resolution`, or `seed` requires re-running the CPU noise loop and re-uploading the VBO. This is a heavier operation:

```js
// TerrainMesh.js
export function rebuildTerrain(params, scene, material) {
  // dispose old mesh to free GPU memory
  if (existingMesh) {
    scene.remove(existingMesh);
    existingMesh.geometry.dispose();
  }
  // build fresh geometry with new params
  const geo = buildDisplacedGeometry(params);
  return new THREE.Mesh(geo, material);
}
```

**Uniform updates** — toggling biome colors, wireframe, or auto-rotate just updates a uniform value or material property. No geometry rebuild needed. The GPU picks up the new value on the next draw call.

**What connects to the next step**
The scene is fully interactive. The final step adds the debug panel — a live readout of internal WebGL state — which makes the invisible parts of the pipeline visible.

---

### Step 8 — Debug Panel and WebGL State Inspection

**What you're building**
Build `TerrainInspector.js` that reads live WebGL state through the renderer and displays it in `DebugPanel.jsx`. Show: FPS, vertex count, triangle count, min/max height range, active uniform values, GPU memory estimate.

**What this teaches**
Everything so far has been about writing into the pipeline. This step is about reading it back out — verifying that what you intended actually happened on the GPU.

Three.js exposes the raw WebGL context via `renderer.getContext()`. Once you have `gl`, you can query state directly:

```js
// TerrainInspector.js
export function inspect(renderer, geometry, material) {
  const gl = renderer.getContext();

  return {
    fps:           /* measure with performance.now() */,
    vertices:      geometry.attributes.position.count,
    triangles:     geometry.index.count / 3,
    drawCalls:     renderer.info.render.calls,
    minHeight:     material.uniforms.uMinHeight.value,
    maxHeight:     material.uniforms.uMaxHeight.value,
    activeTexture: gl.getParameter(gl.ACTIVE_TEXTURE),
    viewport:      gl.getParameter(gl.VIEWPORT),
  };
}
```

`renderer.info` is Three.js's built-in stats object. It tracks draw calls, triangle count, texture count, and geometry count per frame — exactly what you want for understanding what's happening on the GPU side.

**What connects to the next step**
You now have full observability of the pipeline. The optional advanced step moves noise computation from the CPU loop into the vertex shader itself — eliminating the JS loop entirely and doing all displacement on the GPU.

---

### Step 9 (Advanced) — GPU Noise: Moving Displacement Into the Vertex Shader

**What you're building**
Port the Perlin fBm from `fbm.js` into `terrain.vert.js` as GLSL. Pass `uScale`, `uOctaves`, `uPersistence`, `uLacunarity`, `uAmplitude`, and `uSeed` as uniforms. Remove the CPU vertex loop from `TerrainMesh.js` entirely. The geometry becomes a permanently flat static grid.

**What this teaches**
This is the conceptual inversion. In Steps 3–8 the CPU computed heights and uploaded them via the VBO. In this step the VBO stays flat forever — it's just a grid of X/Z coordinates. The Y displacement happens inside the vertex shader for every vertex in parallel on the GPU.

```glsl
// terrain.vert.js (GPU noise version)
// ... GLSL Perlin noise functions ...

void main() {
  float h   = fbm(position.xz * uScale, uOctaves, uPersistence, uLacunarity) * uAmplitude;
  vec3 displaced = vec3(position.x, h, position.z);

  vHeight   = h;
  vNormal   = computeNormal(position.xz);  // finite differences
  vPosition = (modelViewMatrix * vec4(displaced, 1.0)).xyz;

  gl_Position = projectionMatrix * modelViewMatrix * vec4(displaced, 1.0);
}
```

The tradeoff: GPU noise is faster at high vertex counts (the JS loop is gone), but you lose `computeVertexNormals()` — you have to compute normals analytically in the shader using finite differences (sampling noise at neighboring points to estimate slope).

Compare performance and visual output between the CPU path and the GPU path. At `resolution = 128` the difference is negligible. At `resolution = 512`, the GPU path stays smooth while the CPU path hitches on every rebuild.

---

## WebGL Concepts Map

| Concept | Where it appears | What it teaches |
|---|---|---|
| WebGL context | `SceneManager.js` Step 1 | The GPU remote control |
| VBO / EBO | `PlaneGeometry` Step 2 | How vertex data lives in GPU memory |
| Buffer mutation | `positions.setY()` Step 3 | Writing from CPU into a GPU buffer |
| `needsUpdate` | Step 3 | The flag that triggers VBO re-upload |
| Vertex normals | `computeVertexNormals()` Step 3 | Why lighting breaks after displacement |
| Vertex shader | `terrain.vert.js` Step 4 | Per-vertex GPU program |
| Fragment shader | `terrain.frag.js` Step 4 | Per-pixel GPU program |
| Varyings | `vHeight`, `vNormal` Step 4 | Interpolated data from vertex → fragment |
| Uniforms | `uMinHeight`, `uSunDir` Step 5-6 | JS → shader communication |
| Lambertian diffuse | `dot(vNormal, uSunDir)` Step 6 | The core lighting equation |
| Geometry disposal | `geometry.dispose()` Step 7 | Freeing GPU VRAM |
| State inspection | `gl.getParameter()` Step 8 | Reading the GPU pipeline back out |
| GPU noise | `terrain.vert.js` Step 9 | Full vertex-stage displacement |
